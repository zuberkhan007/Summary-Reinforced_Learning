{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL (Reinforcement Learning)\n",
    "* MDP (Markov Decision Process)\n",
    "* Q Learning\n",
    "* DQN (Deep Q Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shorcut acronym: AREAs 51.\n",
    "* Entities of a RL program are: Action Reward Environment Agent states\n",
    "* Goal: Maximise Cumulative Reward or Maximise expected discounted return of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP\n",
    "* S = States, A = Actions, R = Reward. Each has finite number of elements\n",
    "* At time = t: State Action pair = (S_t,A_t)\n",
    "* Reward receiving function = f( S_t,A_t ) = R_t+1\n",
    "* Hence sequence = S_0,A_0,R_1,S_1,A_1,R_2,... \n",
    "* Return = Sum of future rewards:\n",
    "    * Discount Rate = Y (gamma) 0<Y<1\n",
    "    * G_t = R_t+1 + (Y)x(R_t+2) + (Y^2)x(R_t+3) + ... + (Y^k)x(R_T), (T = final time step) OR\n",
    "    * G_t = R_t+1 + (Y)x(G_t+1)\n",
    "    * Influence of future rewards diminishes drastically, hence make hay while the sun shines\n",
    "    \n",
    "###### value function --> expected return --> the way the agent acts --> policy\n",
    "* Policies (pi) : Probability that an agent will select an action from a state\n",
    "* Value Function: How good is an action or state for the agent following a policy (pi)\n",
    "    * State value function (v_pi): Value of state following a policy pi\n",
    "    * Action value function: Value of action following a policy pi\n",
    "        * Q function = Expectation of Q value:\n",
    "            * q_pi (s,a) = E ( G_t ) where S_t = s, At = a\n",
    "\n",
    "* Optimal Policy: \n",
    "    * Expected return is: v_pi (s)\n",
    "    * pi > pi' iff v_pi (s) >= v_pi' (s) where s = state\n",
    "* Optimal Value function:\n",
    "    * Optimal State Value function:  \n",
    "        * v_* (s) >= max( v_pi (s) )\n",
    "        * Hence v_* (s) = larget expected return achievable by any policy pi for each state\n",
    "    * Optimal Action Value function: \n",
    "        * Optimal Q value = q_* (s,a) = max( q_pi (s,a) )\n",
    "        * Hence q_* (s,a) = larget expected return achievable by any policy pi for each state-action pair\n",
    "        * q_* (s,a) MUST satisfy Bellman optimality equation \n",
    "        * Loss = q_* (s,a) - q (s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "* Visualised using a Q table, rows = states and columns = actions\n",
    "* Epsilon greedy strategy: Exploration vs Exploitation\n",
    "    * 0 < epsilon < 1, 0 < learning rate < 1\n",
    "    * At the start of a program: epsilon = 1 = Exploration mode\n",
    "    * Then epsilon decreases below 0.5 = Exploitation mode\n",
    "    * Optimal Q value = q_* (s,a) = max( q_pi (s,a) )\n",
    "    * Loss = q_* (s,a) - q (s,a)\n",
    "    \n",
    "    \n",
    "# Deep Q Learning\n",
    "* Multiple states --> Neural Network --> Multiple q values, q(s,a_n)\n",
    "* In case of image input (screenshots of states in a video game) : image is first grayscaled, resized and cropped --> CNN\n",
    "* Q values suggest what next action to take\n",
    "* Experience replay: \n",
    "    * e_t = ( s_t, a_t, r_t+1, s_t+1 )\n",
    "    * Current state, Current action, Next state reward, Next state\n",
    "    * Random (to eliminate any correlation problem) experience is chosen as input to a neural network\n",
    "    \n",
    "* Loss calculation between output Q value and target Q value: \n",
    "    * Two Forward passes and then weights and biases are updated\n",
    "    * At e = ( s, a ,r', s' ): \n",
    "        * s ---> NN ---> q(s,a_a) + q(s_1,a_b) + q(s_1,a_c) + q(s_1,a_d)\n",
    "        * s' ---> NN ---> q(s',a'_a) + q(s',a'_b) + q(s',a'_c) + q(s',a'_d)\n",
    "        * Calculate Loss, Update weights in NN to minimize loss\n",
    "* Problem: s and s' share the same NN and have only one step between them ==> Optimization is chasing its own tail ==> Unstable\n",
    "* Solution: \n",
    "    * Use 2 separate NN: 1) Target Network 2) Policy Network\n",
    "    * At e = ( s, a ,r', s' ): \n",
    "        * s ---> Policy NN ---> q(s,a_a) + q(s_1,a_b) + q(s_1,a_c) + q(s_1,a_d)\n",
    "        * s' ---> Target NN ---> q(s',a'_a) + q(s',a'_b) + q(s',a'_c) + q(s',a'_d)\n",
    "        * Calculate Loss, Update weights in Policy NN to minimize loss\n",
    "        * After x time steps update weigths in target NN to weights in policy NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Formulae:\n",
    "* exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(exploration_decay_rate*episode)\n",
    "\n",
    "* q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.0\n",
    "# !pip install gym\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Cart Pole\n",
    "* Create DL model with NN\n",
    "* Build Agent with Keras-RL\n",
    "* Reload agent from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gym \n",
    "import random\n",
    "\n",
    "# define E,S,A\n",
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the env without DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:30.0\n",
      "Episode:2 Score:35.0\n",
      "Episode:3 Score:29.0\n",
      "Episode:4 Score:11.0\n",
      "Episode:5 Score:13.0\n",
      "Episode:6 Score:20.0\n",
      "Episode:7 Score:29.0\n",
      "Episode:8 Score:14.0\n",
      "Episode:9 Score:19.0\n",
      "Episode:10 Score:12.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(states, actions):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(1,states)))\n",
    "    model.add(keras.layers.Dense(24, activation='relu'))\n",
    "    model.add(keras.layers.Dense(24, activation='relu'))\n",
    "    model.add(keras.layers.Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: 1.0000\n",
      "done, took 47.833 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29add4e1b20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 199.000, steps: 199\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "# visualize_test = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Cart_Pole_DQN_Weights.h5f', overwrite=True)\n",
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 190.000, steps: 190\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.load_weights('Cart_Pole_DQN_Weights.h5f')\n",
    "scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "# visualize_test = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Atari Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]\n",
    "# !pip install gym keras-rl2 gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym \n",
    "# import random\n",
    "\n",
    "# env = gym.make('SpaceInvaders-v0')\n",
    "# height, width, channels = env.observation_space.shape\n",
    "# actions = env.action_space.n\n",
    "# env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the env without DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 5\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0 \n",
    "    \n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = random.choice([0,1,2,3,4,5])\n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         score+=reward\n",
    "#     print('Episode:{} Score:{}'.format(episode, score))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DL Model\n",
    "* States are interpreted using screenshot of game so CNN is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# def build_model(height, width, channels, actions):\n",
    "#     model = Sequential()\n",
    "#     model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "#     model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "#     model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(actions, activation='linear'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(height, width, channels, actions)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl.agents import DQNAgent\n",
    "# from rl.memory import SequentialMemory\n",
    "# from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "# def build_agent(model, actions):\n",
    "#     policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "#     memory = SequentialMemory(limit=1000, window_length=3)\n",
    "#     dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "#                   enable_dueling_network=True, dueling_type='avg', \n",
    "#                    nb_actions=actions, nb_steps_warmup=1000\n",
    "#                   )\n",
    "#     return dqn\n",
    "\n",
    "# dqn = build_agent(model, actions)\n",
    "# dqn.compile(Adam(lr=1e-4))\n",
    "# dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "# # visualize_test = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.save_weights('Atari_SpaceInvaders_DQN_Weights.h5f', overwrite=True)\n",
    "# del model\n",
    "# del dqn\n",
    "# del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('SpaceInvaders-v0')\n",
    "# height, width, channels = env.observation_space.shape\n",
    "# actions = env.action_space.n\n",
    "# model = build_model(height, width, channels, actions)\n",
    "# dqn = build_agent(model, actions)\n",
    "# dqn.compile(Adam(lr=1e-4))\n",
    "# dqn.load_weights('Atari_SpaceInvaders_dqn_weights.h5f')\n",
    "# scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "# # visualize_test = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Custom Environment for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.0\n",
    "# !pip install gym\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Temperature array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start temp\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Set shower length\n",
    "        self.shower_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 temperature\n",
    "        # 1 -1 = 0 \n",
    "        # 2 -1 = 1 temperature \n",
    "        self.state += action -1 \n",
    "        # Reduce shower length by 1 second\n",
    "        self.shower_length -= 1\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.state >=37 and self.state <=39: \n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        # Check if shower is done\n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Apply temperature noise\n",
    "        #self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Reset shower time\n",
    "        self.shower_length = 60 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19.890383], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ShowerEnv()\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the env without DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-44\n",
      "Episode:2 Score:-58\n",
      "Episode:3 Score:-34\n",
      "Episode:4 Score:-40\n",
      "Episode:5 Score:-48\n",
      "Episode:6 Score:-18\n",
      "Episode:7 Score:-58\n",
      "Episode:8 Score:-32\n",
      "Episode:9 Score:-14\n",
      "Episode:10 Score:-12\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 24)                48        \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zuber\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 31:14 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zuber\\anaconda3\\lib\\site-packages\\rl\\memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 58s 6ms/step - reward: -0.4842 0s - r\n",
      "done, took 57.660 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ae24adf10>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -54.000, steps: 60\n",
      "Episode 2: reward: -56.000, steps: 60\n",
      "Episode 3: reward: -60.000, steps: 60\n",
      "Episode 4: reward: -60.000, steps: 60\n",
      "Episode 5: reward: -54.000, steps: 60\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "# # visualize_test = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Custom_RL_DQN_Weights.h5f', overwrite=True)\n",
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# actions = env.action_space.n\n",
    "# states = env.observation_space.shape[0]\n",
    "# model = build_model(states, actions)\n",
    "# dqn = build_agent(model, actions)\n",
    "# dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# dqn.load_weights('Custom_RL_DQN_Weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "# # visualize_test = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.15.0 tensorflow-gpu==1.15.0 stable_baselines gym box2d-py --user\n",
    "# !pip install stable_baselines gym box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym \n",
    "# from stable_baselines import ACER\n",
    "# from stable_baselines.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the env without DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 5\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0 \n",
    "    \n",
    "#     while not done:\n",
    "#         # env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         score+=reward\n",
    "#     print('Episode:{} Score:{}'.format(episode, score))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DummyVecEnv([lambda: env])\n",
    "# model = ACER('MlpPolicy', env, verbose = 1)\n",
    "# model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_policy(model, env, n_eval_episodes=10, render=False)\n",
    "# # evaluate_policy(model, env, n_eval_episodes=10, render=True)\n",
    "# env.close()\n",
    "\n",
    "# model.save(\"ACER_model\")\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ACER.load(\"ACER_model\", env=env)\n",
    "# obs = env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, done, info = env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Gamestop Trading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu==1.15.0 tensorflow==1.15.0 stable-baselines gym-anytrading gym\n",
    "# !pip install stable-baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gym stuff\n",
    "# import gym\n",
    "# import gym_anytrading\n",
    "\n",
    "# # Stable baselines - rl stuff\n",
    "# from stable_baselines.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv('gmedata.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info {'total_reward': -212.27000000000007, 'total_profit': 0.20090222509186292, 'position': 1}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5xcdXn/38/szt6S7CabbOKGZHcpjSgQDDUiP/ACBgrSItLWNrhgtLRbLbaitgpuq6BdqlaF3tDftqCRDPCjigpWrRABpV4wIBLDRaJkQ0xIllw22extdvb5/XG+M3tmds7s7GV2zs4879drXjPne27Pd+bM5zzn+T7f71dUFcMwDKO0iBTbAMMwDGP2MXE3DMMoQUzcDcMwShATd8MwjBLExN0wDKMEMXE3DMMoQUzcfYiIishvF9uO6SIi54rInmLbYRQfEfmSiPzDHJxHROSLInJYRB4VkdeLyLOFPq8xOfNC3EWk3/caE5FB33J7wD6zKnQi8pCIDLlzviQi94hI82wdPwyIyN+KyC9E5JiIPC8if5ux/hMisl1ERkXk+ox1vycij4jIERF5UUT+Q0QWBZxnuYjcKSJ7RaRPRP5XRF7rW/+RjN980P3uy9z6ahG5TUSOunN9IOP460TkMREZcO/rMta/3+3X545T7VvXKCJfE5HjItIjIm+f9heave5f8NVrRETivuVv59hvl4icP0s2vFNEEu6cR0XkCRH5/Wke7nXABcAqVT1TVX+gqifPxG4R2SAiz7jf70ERaQ3YrlpEbnW/0zER+ZmIvDnfY7kb06dE5KB7fVpExLf+bHfDOiYiT4rI6zKO/XZ37uMi8nURafSt+4yIPOf2fUZE3jGV72BWUNV59QJ2Aefnsd25wJ4pHluB3w5Y9xDwZ+7zYuC7QKyI30PlbNQ5Y/8PAb8DVAInAz3ARt/6TcCbgW8A12fs+3bgIqAOWAJ8G/hCwHl+C/gA0AxUAB3AS8DCgO2vB77nW/5H4AfuPK8EXgQucuuqnN3vB6qBv3bLVW79hcB+4FS3/0PAJ33HvhP4f8BCPOHqA04t0G94PbBlNq973/ZfAv4hYN07gUfc5wjwV8AA0JjPdZax/orksWbJ7mXuO38bUAP8E/DjgG0XuO+wzdXj94FjQFs+xwL+AngWWAWcADwFvNuta3TX5NvcNXoFcBhY4taf6s71Bnet3AHc5Tv2DcArnF2vdfueXYjrKPC7nMuTzYrBvovF/XlvBva6182ubAEwCIwB/e61EjgT+BFwBNgH/FvyT++Ol5e4u+W/BHb4ll8B3A8cchfMH7vyE935Im75P4EDvv22ANe4z+8CnnYXza+Bv/Btdy6wB/gwnpjdDtTi/YkPuwvzb5mBuGep878A/5qlfAsZ4p5lmz8Atk/hXEeBV2cpF+BXwCZf2W+A3/UtfyL5xwJ+160X3/rdjIv/HcCNvnUbgBfd5wXACPBy3/rb8Yn/LF/L1+MTd+AtwA53vTwEvNJnw5i7pvuBD7ny/3LXQh/wfXw3IfIUd1+9FVjvbPqK+42PAn+G99+5113bO4E/d/tdBQwBCWfXDfgcjCC7J/lOOoAfZtg2CLwiz+/0SeAP8zkW8EOgw7f+Kpz4490odmQc+5fAVe7zjcAdvnUnuWtnUYBd9wIfLMR1FPSaF2GZHHQCZwHrgFfhifffqepxPA9zr6oudK+9eBfh+/Hu6P8H74/9l1M9qYgsxROvnW55AZ6w3wEsBy4HbhGRU1X1ebw/yRlu99cD/SLySrf8BuBh9/kA3kVVjyf0N4nI7/hO/TI8j6IV78L9GN5FdRKeR7opw85bROSWqdbP7SvO1h3T2R+vXnnt68ImVbjvM4PXAyuAr7ptl+CJzc992/wcz5PCvT+p7h/leDJjfea+K9xv+nIgoaq/DDj2lPA/4uex7cvxnhquAZqAbwH3iUiVql6Jd4O6xF3Ln3a7fRtYg3fNPQ7EpmFjJZ6A9wPPueJL8QR+sTvmnXiOxUrgj4AbRWSDqt4KvBv4kbPrY/5jB9ntQhxB4a6038f9l39FHr+BiKzA+w2T191kx8p2LSTXiXulnQI4LeDYv8I5BlnsqgVew/T/S9Nivot7O/BxVT2gqr14nsOVQRur6mOq+mNVHVXVXcD/Bd44hfP9i4j04T2uLcN7nAVPkHep6hfdsR/HE6M/cusfBt4oIi9zy19xyyfiCfnPnX3/raq/Uo+H8UI/r/edfwz4mKoOq+og8MdAl6oeUtUX8Dxtf33/UlWnfPNyXI93fXxxqjuKyAV4N5qP5rFtPZ6Hd4Oq9mXZZBPwFVXtd8sL3bt/2z5gkW995nFyrU9+XpTHvpm2V4vIJ0XkVy62/CkROVVEWkWki/TfbjL+BPhvVb1fVePAZ/CezM4O2kFVb1PVY6o6jPd7vUpEGvI831kicgTP878cuMz3/f9IVb+uqmN41/nrgA+r6pCqPoH39Bn4P5sMVT1dVe8IWD2l3yCJiETxbkSbVfWZPI+V7VpY6G7KPwRWisjlIhIVkU14TlTdNOz8At5//H9y1WG2me/ivhIvnpqkx5VlRUReLiLfdI1pR/EerZZN4Xx/raoNwOl48dpVrrwVeK14jYlH3J+mHc/TBk/cz8XzZr+P98j9Rvf6gfsTISJvFpEfi8ghd4yLM+zrVdWhjPq/kFH/vJD0RssvZKx7L/AO4PeccOSNiJyF9wTzRxkecLZta4H78B6F/zFg/duAzb7ipMjX+8rq8UJZyfX+dZOtT34+lse+mbwWOA6sxfttR4BvAt8D4ngCkS9p17K7Jl7AiwVPQEQqfDeWo3jhSsj/ev6xqi5W1WWqepaqPuBb57+mVgKHVNX/HfQE2TULTPU3QEQieA7CCPDeKRwr27XQ75yrg3hPMB/Aa6O5CHgA7wkmbztF5J/wvP0/zniaLDjzXdz34glrkhZXBl4MMZPPA88Aa1S1HvgIEx+9JkVVtwP/APy7u8u/ADzs/izJ10JVfY/b5WE8L+5c9/kR4Bw8cX8YPC8Qz9v/DLBCVRfjPZr77cus0z5gtW+5ZQp1uNEXsnp3slxE/hS4FtigqlPKNhKRM/Bii3+qqlsn2bYa+DpefPwvAjb7A7w470M+uw/j1ftVvu1exfgj7w7g9IyQyOkZ6zP33e/+zL8EKkVkTcCxM3lEVT+hqgOqultV/15VT1TVk1T1elUdDdgvG2nXsrN/Nd73AxN/+7fjic/5QANeoyJM43rOgv9ce4FGSc98avHZNZVj5UPa7+NCnicR8Bu47+lWvNDdH7qnnnyPle1aSJ1HVR9W1deoaiPek8rJwKMBx/4tvPa+X/rKbsALD/+uqh7No+6zynwX9zuBvxORJvHS5D6K1xAE3t12acZj6iK8+He/iLwCeA/TZzNerPMteN7ay0XkSvcIFxWR1yTj6qr6HF5DzhXA990PvR/4Q8bj7VV4F0cvMOpSun53EhvuBq4TkSUisorxMNG0EC+t9EbgAlX9dZb1URGpwbtuKkWkRkQq3LrTgO8Af6Wq901yniheaGoQeEfyySULm4AvZ/F4voz3uy9xv+Of4zUggncjSAB/7cImSU/ue759rxKRU1z8/u+S+7qY7D3Ax0VkgYicgyegt2czLofd0+Fu4PfES92LAh8Ehhn3/vfjZRklWeTWH8QLFdw4i7akcOG+HwL/6H7v0/EaHvON72faPRlfA04TkT9019pH8dpQngnY/vN4GVOXuFDlVI71ZeADInKCiKzE+86/lNxZRM5w13w9ntO1R1WToZUYcIl4ef0LgI8D9ySfcETkOrwb8AXOcZh7srWyhvlFerZMDV6ceZ97/QtQ49v2NryL/wje4+Ub8Dz3frxUuo+TnjWQd7aMK/swsM19Phn4bzxxPognJut8294JPO9b/gzeI1ylr+xqvD/DETxBuQuX8UCWNEe8P/WX3fYTsmXwYn1Z0xED6vg8Xjih3/f6gm/9l9x35H+90637IunZSf2kZxOlbMF7YlG89Dv/9q/3bX8CMJrt98C7Cd6Gd6PeD3wgY/0ZwGN4N4/HgTMy1icftY86u6t96xrxniiO4zUGvr2A1/L1pGfLXOZ+xz68m74/++VSZ88R4G/wYr7fcNdQD14YLXX9MoVsmVw2ubJVeA7MIbwGyXcHHSvzOs2025XtANpzfC/n4/1PB/H+d22+dR8Bvu0+t7o6D2VcR+15HkuAT7t6HXKf/VlWd7rfog8vPXZ5hp1vd3U77n6LRt86xbv5+u36SKGupWwvcYYYhmEYJcR8D8sYhmEYWTBxNwzDKEFM3A3DMEoQE3fDMIwSxMTdMAyjBDFxNwzDKEFM3A3DMEoQE3fDMIwSxMTdMAyjBDFxNwzDKEFM3A3DMEoQE3fDMIwSxMTdMAyjBDFxNwzDKEFM3A3DMEoQE3fDMIwSxMTdMAyjBKkstgEAy5Yt07a2tmKbYRiGMa947LHHXlLVpmzrQiHubW1tbNu2rdhmGIZhzCtEpCdonYVlDMMwShATd8MwjBLExN0wDKMEmVTcRaRGRB4VkZ+LyA4RucGVXy8ivxGRJ9zrYt8+14nIThF5VkQuLGQFDMMwjInk06A6DLxJVftFJAo8IiLfdutuUtXP+DcWkVOAjcCpwErgARF5uaomZtNwwzAMI5hJPXf16HeLUffSHLtcCtylqsOq+jywEzhzxpYaoSK2PUbbzW1EbojQdnMbse2xYptkGIaPvGLuIlIhIk8AB4D7VfUnbtV7ReRJEblNRJa4shOAF3y773FlRokQ2x6j474Oevp6UJSevh467uswgTeMEJGXuKtqQlXXAauAM0XkNODzwEnAOmAf8Fm3uWQ7RGaBiHSIyDYR2dbb2zst443i0Lm1k4H4QFrZQHyAzq2dRbLIMIxMppQto6pHgIeAi1R1vxP9MeA/GA+97AFW+3ZbBezNcqxuVV2vquubmrJ2sDJCyu6+3VMqNwxj7sknW6ZJRBa7z7XA+cAzItLs2+wy4Bfu873ARhGpFpETgTXAo7NrtlFMWhpaplRuGMbck4/n3gw8KCJPAj/Fi7l/E/i0iGx35ecB7wdQ1R3A3cBTwHeAqy1TprTo2tBFXbQurawuWkfXhq4iWWQYRiaimivxZW5Yv3692tgy84vbf76Fd93zARKRl2htaKFrQxfta9uLbZZhlBUi8piqrs+2LhQDhxnzj8tesZG/H17CuSc38aV3WaarYYQNG37AmBZDcS/SFk+MFdkSwzCyYeJuTIukuI+MmrgbRhgxcTemxbATdRN3wwgnJu7GtEh67sMm7oYRSkzcjWkxFPdE3WLuhhFOTNyNnAQNEDY86mLuJu6GEUosFdIIJDlAWHIcmeQAYQAroxcAFnM3jLBinrsRSK4BwsZTIYvfCc4wjImYuBuB5BogbGjUUiENI8yYuBuB5BogbDhuqZCGEWZM3I1AujZ0UVNZm1aWHCAs1YkpMUYYxicyDCMdE3cjkPa17fz9Of9MxVgTILQ2tNJ9STfta9sZ8nnsFnc3jPBh2TJGTt5wwmWsGl7Jn6xfzaf+6PRUeTIsA573XlVpfoJhhAn7Rxo56R8eBWAgnj4kf7JBFSzubhhhxMTdyElS3AdHMsQ9buJuGGHGxN3IyXEn7kOZnnvcH3M3cTeMsGHibuQk5blniPuwLyxjg4cZRvgwcTdyEhSWSWtQNXE3jNBh4m7k5HiA5+4P01hYxjDCh4m7kZP+oQDPfTQ9FdIwjHAxqbiLSI2IPCoiPxeRHSJygytvFJH7ReQ5977Et891IrJTRJ4VkQsLWQGjsATF3IfiCSojAlhYxjDCSD6e+zDwJlV9FbAOuEhEzgKuBbaq6hpgq1tGRE4BNgKnAhcBt4hIRSGMNwpPoLiPJqivjQIm7oYRRiYVd/Xod4tR91LgUmCzK98MvNV9vhS4S1WHVfV5YCdw5qxabcwZyZj7yOgYibHxYQaG4mPU13gdnC0sYxjhI6+Yu4hUiMgTwAHgflX9CbBCVfcBuPflbvMTgBd8u+9xZZnH7BCRbSKyrbe3dyZ1MArIMSfukO69D48mWFRjnrthhJW8xF1VE6q6DlgFnCkip+XYXLIdIssxu1V1vaqub2pqys9aY8457hd3X6PqUHyMRUnP3cTdMELHlLJlVPUI8BBeLH2/iDQDuPcDbrM9wGrfbquAvTO21CgKx4cTNLjYuj/9cSieoD7puVtYxjBCRz7ZMk0isth9rgXOB54B7gU2uc02Ad9wn+8FNopItYicCKwBHp1tw43CMzam9A+P0rSoGsgMy4xRX+t57pbnbhjhI58hf5uBzS7jJQLcrarfFJEfAXeLyFXAbuBtAKq6Q0TuBp4CRoGrVTURcGwjxCRHgmxaWM3OA/2psMzYmDIyOmYxd8MIMZOKu6o+CZyRpfwgsCFgny6ga8bWGUUl2YFpmfPcB5y4JzswJWPuNraMYYQP66FqBJLMcV+2sAoYj7kn35Oeu4VlDCN8mLgbgSTFPTPmnvTUa6MVRCvEwjKGEUJM3I1AkmmQTQuduI+ke+7VlRGqKiIm7oYRQkzcjUAyPfdkA2tyir2aaAXRyoilQhpGCDFxNwJJNag6z30o2aDqxnKviXqeu8XcDSN8mLgbgRwf8cR9eUbMPRmWqYlWUFUZsWwZwwghJu5GIMec515fG6WqIjIu7k7MqysjVFVazN0wwoiJuxHI8eFRKiNCdWWEmmhkQoNqTbTCwjKGEVJM3I1A+odHWVhTiYhQW1WREvdkGKYmap67YYQVE3cjkP7hURZUeb1Qa6MVE2Lu1ZWe527ZMoYRPkzcjUD6h0ZTQwzU+MR9OCnu0QhRy3M3jFBi4m4EcnxklAXVnrjXVVWkPPbxsIyXLTOSmDBcv2EYRcbE3QikfziREnd/zD3VoFpZYTF3wwgp817cgzI1THBmTv9QnEXV4zH3gZS4jxERiFaIE3cb0dkwwsa8FvedB/p502cf4n93vpRW/uAzB1j38e/ylcf2FMmy0uD4cIIF1RWAF4LxjwpZXVmBiFiDqmGElHkt7jXRCLXRCjbd9ih3/9Sbk/tb2/fRcfs2BkYSfOOJ3xTZwvlN//AoC6u9YX392TLDo2PURL1Lp6oiQnzUYu6GETbymYkptKxaUsdX3nM2V8ce50NffZLvPXOA7z71Ir/TsoSTmhbytZ/9hoGRUeqq5nU1i8LYmHJ8ZJSFznOvq0pPhayJeuVVNnCYYYSSee25A9TXRLntna/h8jNb+M6OFzn7pGV8+aozecu6lYwkxvjhzoPFNnFeMhBPoEqqQbWmyhdzHx2jutJ57tagahihpCRc2mhFhBsvO40/Xr+KU1bWU11ZwWvaGllQVcH3nj3A+aesKLaJ847kWO4La8YbVEdGx0iMKcM+zz1qMXfDCCUlIe4AIsIZLUtSy1WVEV63ZhkPPXMAVUVEimjd/CM5lvtCX7YMeCGZodExqv1hmdEx+44NI2RMGpYRkdUi8qCIPC0iO0Tkfa78ehH5jYg84V4X+/a5TkR2isizInJhISuQi/NOXs7eviF+ub+/WCbMW5JjuS/05bmDN+zvUDxBjQvLJMMzcevIZBihIh/PfRT4oKo+LiKLgMdE5H637iZV/Yx/YxE5BdgInAqsBB4QkZer6pwnQ5978nIAHnz2ACe/bNFcn35ekwzLLMjw3AdHEgzHEzTUeZNmRys8b30kMUZV5bxvwjGMkmHSf6Oq7lPVx93nY8DTwAk5drkUuEtVh1X1eWAncOZsGDtVXtZQwyub63nwmQPFOP285lhmWMbnuQ+PjqU896oK57lbo6phhIopuVoi0gacAfzEFb1XRJ4UkdtEJBnwPgF4wbfbHrLcDESkQ0S2ici23t7eKRueL+ed3MS2nsMcHYoX7BylyPGAmPvgSCIjFdJ7t0ZVwwgXeYu7iCwEvgpco6pHgc8DJwHrgH3AZ5ObZtl9QkBWVbtVdb2qrm9qapqy4fly3iuWkxhTHnnupck3NlL0B4Vl4gmG4r5OTM6Dt3RIwwgXeYm7iETxhD2mqvcAqOp+VU2o6hjwH4yHXvYAq327rwL2zp7JU+OM1YuprozwxAtHimXCvCQp7qkhf9PCMt7wAzAec7d5VA0jXOSTLSPArcDTqvo5X3mzb7PLgF+4z/cCG0WkWkROBNYAj86eyVOjsiLCivoa9h8dKpYJ85Ljw6NUuCn2wOuhCjA0ku65j2fLmLgbRpjIJ1vmHOBKYLuIPOHKPgJcLiLr8EIuu4C/AFDVHSJyN/AUXqbN1cXIlPGzfFE1B44OF9OEeUf/0CgLqytTuevJsMzASIKh0fThB8DCMoYRNiYVd1V9hOxx9G/l2KcL6JqBXbPKivoann7xaLHNmFf0DydSjakwLu59g3FUxz32qgprUDWMMFIWiclNi6rpNc99SvQPx1PD/cJ4zP3IoJd1ND78gHfft1RIwwgXZSHuK+prODY8mkrvMybneIDnfmRgBCBt+AGAYfPcDSNUlIm4VwNw4Jh57/lybHh8/lTwBgiLVgiHB5znXmmpkIYRZspC3JcvqgHggGXM5M3x4dFUGmSSmmjFRM+9wsTdMMJIWYh70nPfb5573hwfHmVBxiQntdEKDjtxz/TcLRXSMMJFWYi7ee5Tp39oNDWWe5LaqgoOH09vULWwjGGEk7IQ9/raSqorIxZzzxONxfj2zVfy0UvXQlsbxGKA57knwzI1mWEZ89wNI1SUzGQduRAR66WaL7EYdHSwamDAW+7pgY4OAGqr2jjuptpL5rlHzXM3jFBSFp47WC/VvOnsRJLCnmRgADo7U+mQYJ67YYSdshH3FfU17D9mnvuk7N4dWJ4u7unjuZvnbhjhomzE3Xqp5klLS2B5spcqkBoVMhIRohVi4m4YIaNsxN16qeZJVxeJ2tr0sro66OqiLovnDl4HJ0uFNIxwUUbibr1U86K9nZ3/8Dn21DehItDaCt3d0N6emmoPxmPu4KVDmuduGOGiLLJlID3X/cRlC4psTbh54aLL+LMDq7n3vedw+qrFqXJ/zL3aNxl2VUXEGlQNI2SUneduvVQnZzDupTv6xRzSOy4lx3kHLyxjMzEZRrgoG3G3Xqr5kxT3mgxxT4ZlairTL5vqygjxxIRpcg3DKCJlI+7WSzV/hpOee1W6uCen2ssUfS/mXtTJtgzDyKBsxN16qebPZGGZ7OJuYRnDCBNlI+5gvVTzZXDEE+oJYRm3XJ0Rlolag6phhI6yEnfrpZofg/EEVZURKiLpU+fWBnnuFRHioxZzN4wwMam4i8hqEXlQRJ4WkR0i8j5X3igi94vIc+59iW+f60Rkp4g8KyIXFrICU6HJPPe8GIonJoRkwNegGk2/bKoqIzbNnmGEjHw891Hgg6r6SuAs4GoROQW4FtiqqmuArW4Zt24jcCpwEXCLiExUiiKwor6GfuulOimDI4kJAg7j4p4ceiCJxdwNI3xMKu6quk9VH3efjwFPAycAlwKb3Wabgbe6z5cCd6nqsKo+D+wEzpxtw6eD9VLNj8Egzz0a4Lnb8AOGETqmFHMXkTbgDOAnwApV3QfeDQBY7jY7AXjBt9seV1Z0LNc9PwbjiQlxdfA1qFq2jGGEnrzFXUQWAl8FrlHVo7k2zVI2obVNRDpEZJuIbOvt7c3XjBlhvVTzYyiemJDjDv5OTBMbVE3cDSNc5CXuIhLFE/aYqt7jiveLSLNb3wwccOV7gNW+3VcBezOPqardqrpeVdc3NTVN1/4pYZ57fgyOZA/L3Pfc3eypfhefe/oM2m5uI7bdm34vWimWCmkYISOfbBkBbgWeVtXP+VbdC2xynzcB3/CVbxSRahE5EVgDPDp7Jk+fZC9V68iUm2wx99j2GO/7n/eQiPQCSk9fDx33dRDbHqOqooK4ee6GESry8dzPAa4E3iQiT7jXxcAngQtE5DngAreMqu4A7gaeAr4DXK2qoeibLiIsr6+m18IyORmMJ9Im5gDo3NrJQDx9+r2B+ACdWzstFdIwQsikQ/6q6iNkj6MDbAjYpwvomoFdBWPZwmpe6h8pthmhZihLWGZ3X/bp93b37U41qKpq2miRhmEUj7LqoQrQtNA898nIFpZpacg+/V5LQwtVFZ6g28iQhhEeyk/cF1XT22/inovBLNkyXRu6qIvWpZXVRevo2tBFlRtrxnLdDSM8lJ24L1tYzeGBEROiAMbGlKH42IQ89/a17XRf0k1rQyuC0NrQSvcl3bSvbaeqwruMLB3SMMJD2Uyzl6RpUTWqcOj4CCvqa4ptTuhIzqiULRWyfW077WvbJ5RHnedu6ZCGER7KznNvWuR1ZLK4e3aGUmO5539pmOduGOGj7MR92UIn7hZ3z8pgwCxMuagyz90wQkfZifty89xzEjR/ai6Sk3eY524Y4aHsxD3pub9knntWBkeyT7GXi6iFZQwjdJSduNdWVbCwutI89wCGpuG5WyqkYYSPshN3cLnuJu5ZmVbM3Tx3wwgd5SnuC6stLBPAdMIySc/dxpcxjPBQluK+bFGVee4BTKdB1WLuhhE+ylLcbXyZYIamEZaptpi7YYSOshT3ZQurOTo0yvBoKEYiDhUzCcuY524Y4aEsxT3ZS9WG/p3IYDx4+IEgLCxjGOGjrMXdQjMTScbck6GWfLBUSMMIH2Up7qmOTCbuExiKJ6iJRohE8p90I5UtY567YYSGshT3lOdu6ZATCJocOxepPHfz3A0jNJSluC9dWAVYWCYb2WZhmgzrxGQY4aMsxb26soKG2qh1ZMpCtsmxJyMSESojYjF3wwgRZSnuYEMQBJFtcux8SE6SbRhGOJhU3EXkNhE5ICK/8JVdLyK/EZEn3Oti37rrRGSniDwrIhcWyvCZYkMQZGdo1MTdMEqBfDz3LwEXZSm/SVXXude3AETkFGAjcKrb5xYRmbpSzAHLzHPPyuDIxMmxJyUW49ufu5LrLzsd2togFiuIbYZh5M+k4q6q3wcO5Xm8S4G7VHVYVZ8HdgJnzsC+gmFDEGRnMMvk2DmJxaCjg+a+A4gq9PRAR4cJvGEUmZnE3N8rIk+6sM0SV3YC8IJvmz2ubAIi0iEi20RkW29v7wzMmB5Ni6o5PpJgYGR0zs8dZoammi3T2QkDA+llA3f3+JIAABbQSURBVANeuWEYRWO64v554CRgHbAP+Kwrz9bzRbMdQFW7VXW9qq5vamqaphnTZ5lLh3zpmA1B4GdwxOvElDe7d0+t3DCMOWFa4q6q+1U1oapjwH8wHnrZA6z2bboK2DszEwvDeEemoSJbEi6mnOfe0jK1csMw5oRpibuINPsWLwOSmTT3AhtFpFpETgTWAI/OzMTCkByCoNc89zSmnOfe1QV1delldXVeuWEYRSOfVMg7gR8BJ4vIHhG5Cvi0iGwXkSeB84D3A6jqDuBu4CngO8DVqhrKcXVXf/trPPL5d3Hh6Sstw8ORGFNGRsem5rm3t0N3N4ebmhlDoLUVuru9csMwikblZBuo6uVZim/NsX0XEG63LRaj/n1X05BsCExmeEBZi1Jqoo6p5rm3t/Ofy17NFx7+NTu73oxI/oOOGYZRGMqzh2pnJ2IZHhOYzuTYSZbUVZEYU44NW/aRYYSB8hT3jEyO2FpouwYi7+yh7eY2YtvLM0STnIVpSnnujsV1XvbRkePxWbXJMIzpUZ7i7svkiK2FjkugZzGoQE9fDx33dZSlwE87LAMsqYsCcHjAGqgNIwyUp7j7Mjw6N8BAVfrqgfgAnVvLL0QzOANxX2zibhihojzF3WV40NrK7obsm+zuK79OOKnJsacRc0+FZQYsLGMYYaA8xR08gd+1i5bFrVlXtzSUXyecpOc+nZj7kpS4m+duGGGgfMXd0bWhi7poRicchJ6+8mtcnUnMvaE2iggcNs/dMEJB2Yt7+9p2ui/pprXB78F7w+GUW+PqTFIhKyJCfU3UPHfDCAllL+7gCfyua3ZlCLxHsnE1tj1G281tRG6IlKxHPxT3JtuYjucOXsaMee6GEQ4m7aFaTgQ1oiY9+IH4QNoyeDeGUiHVoDpNcV9cV2XZMoYREsxz95GrETUp7P7lUkuXTDWoVk3vslhcF7VsGcMICSbuPrI3rgZTaumSQ/EEEYGqiuldFkvqqjgyaJ67YYQBE3cf2RtXgym1dElvoo6KaQ/8tbguasMPGEZIMHHPINm4KlknlRqnLlpH14ZwD345VaY8UUcGS+qqODY8SjwxNotWGYYxHUzcA8jllVfLCrov6S6pxlRwE3XMSNy9IQgs7m4YxcfEPYBs8fe6aB0dp97EywZu5fUrLwMoqRTJoXhiWjnuSRZbL1XDCA2WChlA0ivv3NrJ7r7dtDS00LWhi7Ob38r/bHuIh5/rJbL3kZJKkRwcmVlYZnzwMPPcDaPYmLjnoH1t+wSRVlVWN9by8LO9PHC4MzBFcl6K+yzE3MFGhjSMMGBhmSkiIrzx5U388FcvBaZCztcUycH42NQmx84g6bn3meduGEXHxH0a/Mkvf8B3//kdrD6iWdfP1xTJoZEEtdHpXxLmuRtGeJj0nywit4nIARH5ha+sUUTuF5Hn3PsS37rrRGSniDwrIhcWyvCiEYtx2g1/y6qjvdy4FeoydGw+p0jONCxTV1VBVUXEYu6GEQLycdO+BFyUUXYtsFVV1wBb3TIicgqwETjV7XOLiExfLcKIb3Lt9u3QfR+0HgFRqBhr4sZz/3VextvBifsMwjIi4oYgMM/dMIrNpOKuqt8HDmUUXwpsdp83A2/1ld+lqsOq+jywEzhzlmwNBxmTa7dvh103Q+LjwqrhL7IgcW5x7JoFhkZmlucOXtzdwjKGUXymG2Bdoar7ANz7cld+AvCCb7s9rmwCItIhIttEZFtvb+80zSgCLdnj6dLSwitetoj7n9o/xwbNHjMNy0ByZEgLyxhGsZntBtVsffaztjqqareqrlfV9U1NTbNsRgHxTa6doq4Ourq44JQV/HTXIQ4fn3+eazwxxuiYzljcl9RFLVvGMELAdMV9v4g0A7j3A658D7Dat90qYO/0zQshvsm1EfHeu7uhvZ23Pft9vn/Lu1i8qAba2iAW4t6qsZhnYyQCbW2M3r4FmN4sTH6W2JjuhhEKpivu9wKb3OdNwDd85RtFpFpETgTWAI/OzMQQ4ibXZmzMe29vh1iM1ddew6qjvYgq9PRAR0c4BT4W82zr6QFna03Hn/H4v1zOVW/87RndmBbXVXFkII5q9jRRwzDmhnxSIe8EfgScLCJ7ROQq4JPABSLyHHCBW0ZVdwB3A08B3wGuVtVEoYwPFb4smhQDA9AZwgk9Ojs923xIPE7j4LEZ35iW1EUZSYwxMFIeP7thhJVJhx9Q1csDVm0I2L4LmJ+J3jNhd0Cv1KDyYpKPTckbU/vU0jr9HZkWVNvoFoZRLKyH6mwRkEUTWF5M8rVpGjemBhv21zBCgYn7bJEjiyZ0ZLM1G9O4MdkQBIYRDkzcZwuXRaMtLYwhHGlqTmXRhA5n64ElKxhDYOlSqKpK32aaNyabsMMwwoGJ+2zS3o709HD+P32Pzpu+GU5hT9LezsUf2MJ1//UEvPQS3HZb1vTOqWITdhhGOLAWrwLQvLiGfX2DxTYjJ8eHR3mpf5iWpS48094+Kzcjm7DDMMKBee4FoLmhln19Q8U2IycvHPZSIVsa84i9T4HoXXfywy/8KX91wcnh78hlGCWMee4FoLmhhv1HhxhNjFFZEc77Z89BT9xbl86iuLvOUSuTOfTJfHkId4jKMEqQcCrPPKe5oZYxhQPHhottSiAvHCqA556lc1RoO3IZRolj4l4AmhfXAIQ6NNNzcID6mspUA+isMJ86chlGiWPiXgCaG5LiHt5G1d2HBsYbU2eL+dSRyzBKHBP3AtDcUAvAviPh9dx3HxqgtXHB7B40S+coDWtHLsMocUzcC0B9TSULqipCG5ZJjCl7Dg+wepYzZfzDIasIe+qb+OlHPmmNqYZRBEzcC4CI0Ly4NrRhmX19g8QTOruZMkmSwyEnEnz59zto/WwX6saMt7RIw5g7TNwLRHNDDXvD4LlnTMpBLMbuQmTKZCB33MGH7vkcKw7vD//49oZRgpi4F4jmhhr2HSmy555lUg5PYO8ACivudHZSOZRRf0uLNIw5w8S9QDQ31NLbP0w8MTb3J09661dckTXv/NR//xSVEUll9RQES4s0jKJi4l4gmhtqUIX9R+c4NOP31gOoP7CPVUtqC9t71tIiDaOomLgXiObFLh1yruPu2XqJZtC7ZPnsZ8pkMp/GtzeMEsTEvUCsdCGPvXMdd58k7DFSVcPnzntnYTJl/PjSIscQXlr6svCOb28YJYiJe4FIeu4vzobnniXjJZAcYY/jixror4hy4z2f5sPvvqjwmSsuLfLa//oZG/5qM2OXv72w5zMMI8WMxF1EdonIdhF5QkS2ubJGEblfRJ5z70tmx9T5xcLqShZVV848LJOR8RKr76HtsSuJ3CC0dS0jdt6yNNE/9tEbGIxWpx+jrg7e8x7qRuM0Dh4jgrJo/945S018TVsjfYNxnjvQX/BzGYbhMRue+3mquk5V17vla4GtqroG2OqWy5LmxTXTDsvEtsdou7mNyHNX0NYxQGwtxNZCxyXQ06Ao0DN6kI6zDxI7bVz0G3a/kxV/U82X39CYPqvSt76FDBZnxMbXnrgUgEd3HSr4uQzD8CjEeO6XAue6z5uBh4APF+A8oae5oZYXp5EtE9seo+O+DgbiAyDQs9gT9do4DGQM4jhQBe+7CAajMFClAByrPsp7Lqyj4t9up32ti3FfeWX2k81BauLqxlpW1Ffz0+cPceVZrQU/n2EYM/fcFfiuiDwmIm5WBlao6j4A9758hueYtzQ31LDXDR6W8sRviNB2cxux7RPDIcltrrjnCk/YfQxUwcGANtCDdVlEPz5A51afV17E1EQR4TVtjfx01yFUteDnMwxj5p77Oaq6V0SWA/eLyDP57uhuBh0ALSWa+/yb4ft5Iv4p5IZeBMELpkBPXw8d93n3wqRnneatzxK7+3xeeVeXF2P3p0nOYWrimSc28s0n97Hn8GDh0zANw5iZ566qe937AeBrwJnAfhFpBnDvBwL27VbV9aq6vqmpaSZmhJLY9hh3//rvSUR6AVLCniTTs+7c2jmpsC8dgLpRSSurG/HKs9HS4Ltp+lIT02Lxc5SauD/+AHuq30Xrvy4MfHIxDGP2kOk+JovIAiCiqsfc5/uBjwMbgIOq+kkRuRZoVNUP5TrW+vXrddu2bdOyI6y03dxGT19wL9EkS2u9xsaDgwdzblcXraP7km7AuxHs7ttNS2UjXQ8ABw/ScakwUKkTtk/F3ItItqeSMNlnGPMVEXnMl8ySvm4G4v5beN46eOGdO1S1S0SWAncDLcBu4G2qmjNNohTFPXJDZIK3Pl1aG1rp2tCVUwhj22Pjot/QMun2c0muG11r5VK6HoD2hw958f+uLu9pIhbzMnl2704vNwwjRUHEfTYpRXHP13PPRal4t5Pd6OpGoPs+aN+O1w6waRNs3jyxfcB6uBpGGrnE3XqoFoiuDV3URdMbDgUJ2HoirQ2tJSHskBH7z8JAFXRu8PL42zoGiCz/fCq3f3wjGy7YMKaCiXuBaF/bTvcl3bQ2tCIIrQ2t3P4Ht9PaMHmed2tDK7uu2VUSwg7Zb3SZ9DS4DlqLQX25/WkCb8MFG0beFKITk+FoX9ueVaBzpTzWRevo2lBaIycmv4POrZ2BoaqKsewdtDo3uHAN2HDBhjEFzHOfYzI9+qW1S1lauzTl3ZdKKCaT9rXt7LpmF1v+YMsEL75uBBIBV+LuBu99JFJB4lh/foOnGYZhnnsxCPLoywG/F59K53wIOtcdpGfxxO1b+iCxpBGOHqXikEsXTU4XCNbAahgBWLaMEQpy5sJf0pl9ZqnWVti1a+6MNIyQYdkyRujJ1gCdClHZfKyGMWUsLGOEhsBwVUtLds/dGlgNIxDz3I3wY/OxGsaUMXE3wo9v0DMV4VDNIhI1tXDllcTOW0Zb17KcQynPGr7pDtPOm2VGLMMoNibuxvzAzcd6pPtWakZHqDjkzUDVcfZBekYPomhqKOXY9lj6+Pn5iO9kwi3iTXjS0zPxvL4ZsejpIXbTu+buhmMYAVi2jDG/aGtLxd/briFr+uTS4zBYnTFKphu/BqDzfGF3g9LoRuQ8NHiQlj6h6wFv+45L0jtUpY19k+O8rUega2uW/edojKC0wePciKHtDx8i9sZGOs+H3aOHJgwqFzjgnA3cNi+wgcOM0iESAXfNRj7mDVUwAYVsw/gsPZ6cjjD7oetGvKkMDy6YuK71COy6mZznFfXy8rMJP4wP73xo8BCNtY2pz7MximfWVNIR2PQz2HxGxs1Gquh+cFHWoaKTk8q0uptd+3ZvCIiJN8TZsbuk8N0Qc91QZxMTd6N0yMNzDxL3wPI8thGFsRvIed7WI16P2qw3nElIiWrAEMiTeeU9o9nnA6hIQKIiu60QfCOC4JtD2jajQvc3FJYunRMxCxvjv0tP7qe/LN/TbNzgTdyN0iEWS00XGFubPYQS5H3PRNz9nnvQebvv88bCySWY+ZAWBhLxYvyZk7HkIby56iPuUJPdiIJuDn6yPRElb1aBTyt5ho3SCAgVBd34aPTOx6EcN8qA8/m38dud+fnYyDFGEiOp/XJdf5M+OU4jfGfibpQW2R5/4wdp6fNi3jBF0feR7Q+YKbaoEjvX56lmCFXHeccY0JHAc+SD/2YS9KSQj/DOxHMHZnRDnIzAsJHzctuPthL78MV0Dn8rzTP2h4p6GjRtfuLkcf1tJCmCbpTJ8+31bkSxlRPDVVNiBk+OyRFh88XE3Sh9fB49pMeJW1yoI1uM2U/WEIPfE8yzYTHp9c1kshZ/GGiqbQup+gSJp69xOfMmmEk+N5Dpinuu46cap98CA9Hx8nyfWPw3Rz+5QmqT3UzzZgbiLghjHxvL+1S5xN16qBqlQVJwnUfffrSF9lf7hDg5z0fA43bq8fwT3vZp8j3FOUKSPW2zNXLmS0tf+uesnvtYsPC2Vi6l6yHvpnTOYGP6zeq+o7Rvj6e2DfSAR4VNjymbX53jhpjnE1EQuUYD7dyQLuzgCXr3+slvOMnRRKdTHrRNviwdyP70l8/3NNnENlPBPHfDKCCTxW4PDh6cNKwQFOPP6pXnE7fNJ3bti0Vnr0Pu9NF8yeW5BzZO5xPeKJLnPvHpL//vyWLuhlFiZG0QfOhgKr4P2cNMU2qMLIjhE9s+ekYn3qyCmCxsFNQ4PVmoKDDmTu7G8Fw30yCikSj11fW5M14CUiQtW8YwypV52pEon0yT7NkyByc0mk6IuQeEirKmkiazZQ4G3CjrlZaj4+dLq0NAbv9s90+YKbnEHVUtyAu4CHgW2Alcm2vbV7/61WoYhqFbtqi2tqqKqLa26pZb3qOtN7WqXC/aelOrbnlyi7fZk1uylud7XN2yZWL50qXeK3ObEANs0wBdLYjnLiIVwC+BC4A9wE+By1X1qWzbm+duGIYxdYoxWceZwE5V/bWqjgB3AZcW6FyGYRhGBoUS9xOAF3zLe1xZChHpEJFtIrKtt7e3QGYYhmGUJ4US96AEpvEF1W5VXa+q65uamgpkhmEYRnlSKHHfA6z2La8C9hboXIZhGEYGhRL3nwJrROREEakCNgL3FuhchmEYRgYFy3MXkYuBm4EK4DZVDZzwUkR6gekPxAHLgJdmsP98pBzrDOVZb6tz+TDVereqata4dig6Mc0UEdkWlA5UqpRjnaE86211Lh9ms942h6phGEYJYuJuGIZRgpSKuHcX24AiUI51hvKst9W5fJi1epdEzN0wDMNIp1Q8d8MwDMPHvBZ3EblIRJ4VkZ0icm2x7SkEIrJaRB4UkadFZIeIvM+VN4rI/SLynHtfUmxbC4GIVIjIz0Tkm265pOstIotF5Csi8oz7zf9PqdcZQETe767vX4jInSJSU4r1FpHbROSAiPzCVxZYTxG5zunbsyJy4VTONW/F3Y08+e/Am4FTgMtF5JTiWlUQRoEPquorgbOAq109rwW2quoaYKtbLkXeBzztWy71ev8z8B1VfQXwKry6l3SdReQE4K+B9ap6Gl7fmI2UZr2/hDccup+s9XT/843AqW6fW5zu5cW8FXfKZORJVd2nqo+7z8fw/uwn4NV1s9tsM/DW4lhYOERkFfB7wH/6iku23iJSD7wBuBVAVUdU9QglXGcflUCtiFQCdXjDlZRcvVX1+8ChjOKgel4K3KWqw6r6PN7cGGfme675LO6TjjxZaohIG3AG8BNgharuA+8GACwvnmUF42bgQ4B/OvhSrvdvAb3AF10o6j9FZAGlXWdU9TfAZ4DdwD6gT1W/S4nX20dQPWekcfNZ3CcdebKUEJGFwFeBa1T1aLHtKTQi8vvAAVV9rNi2zCGVwO8An1fVM4DjlEYoIicuxnwpcCKwElggIlcU16pQMCONm8/iXjYjT4pIFE/YY6p6jyveLyLNbn0zcKBY9hWIc4C3iMguvJDbm0RkC6Vd7z3AHlX9iVv+Cp7Yl3KdAc4HnlfVXlWNA/cAZ1P69U4SVM8Zadx8FveyGHlSRAQvBvu0qn7Ot+peYJP7vAn4xlzbVkhU9TpVXaWqbXi/7fdU9QpKuN6q+iLwgoic7Io2AE9RwnV27AbOEpE6d71vwGtbKvV6Jwmq573ARhGpFpETgTXAo3kfNWhy1fnwAi7Gm6v1V0Bnse0pUB1fh/co9iTwhHtdDCzFa1l/zr03FtvWAn4H5wLfdJ9Lut7AOmCb+72/Diwp9Tq7et8APAP8ArgdqC7FegN34rUrxPE886ty1RPodPr2LPDmqZzLeqgahmGUIPM5LGMYhmEEYOJuGIZRgpi4G4ZhlCAm7oZhGCWIibthGEYJYuJuGIZRgpi4G4ZhlCAm7oZhGCXI/wc+MyiRGDZf7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5)\n",
    "a = env.signal_features\n",
    "b = env.action_space\n",
    "state = env.reset()\n",
    "\n",
    "while True: \n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    if done: \n",
    "        print(\"info\", info)\n",
    "        break\n",
    "        \n",
    "plt.figure()\n",
    "plt.cla()\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_maker = lambda: gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5)\n",
    "# env = DummyVecEnv([env_maker])\n",
    "\n",
    "# model = A2C('MlpLstmPolicy', env, verbose=1) \n",
    "# model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('stocks-v0', df=df, frame_bound=(90,110), window_size=5)\n",
    "# obs = env.reset()\n",
    "# while True: \n",
    "#     obs = obs[np.newaxis, ...]\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, done, info = env.step(action)\n",
    "#     if done:\n",
    "#         print(\"info\", info)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,6))\n",
    "# plt.cla()\n",
    "# env.render_all()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
